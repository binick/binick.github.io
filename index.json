[{"content":"In the previous post, we have seen how to enrich a JWT issued by Active Directory B2C with custom policies.\nIn that post, we talked about how it\u0026rsquo;s possible to add information external to Microsoft Graph to a JWT through the use of a Logic App and a Blob Storage.\nInstead, in this one, we will see how it is possible to create a solution that integrates Azure Active Directory B2C.\nFollowing the track of what we covered in the previous post, we will see how to save on Blob Storage dummy data at user registration.\nNote Throughout the rest of the article, there are references to resources and concepts covered in the previous article to which we refer. \nSolution overview. The solution is composed as follows:\n read-customer-details-identity-la: represents the API whose purpose is to retrieve the content of the blob from customersstgacc (the storage account) customer-register-tpc: is the topic in which are collected the events of the creation of a new user customer-identity-details-filler-la: it represents the API that is in charge of generating fictitious data that will be saved inside a blob on the customersstgacc  contoso-b2c: is the access and identity management service offered by Azure  Introduction to Azure Event Grid. In Azure there is an implementation of the publish/subscribe pattern designed to facilitate integration and resource management via an event-driven development paradigm.\nThrough the Event Grid will be possible to subscribe to built-in message sources via a set of handlers.\nIf this is not enough, it is possible to create custom _topics to which you can subscribe to receive events.\nCreating a custom topic. You can refer to this guide to create a topic.\nOne choice to make when creating the topic concerns the scheme of the HTTP request content used. Currently, supported schemas are:\n Event Grid Schema Cloud Event Schema Custom Input Schema this schema will require the creation of an association between the properties of the input object and those required by the Event Grid Schema  The message used in this case has the following structure\n1[ 2 { 3 \u0026#34;data\u0026#34;: { 4 \u0026#34;objectId\u0026#34;: \u0026#34;25100647-0dcc-4571-b7b4-b03e4ce72d02\u0026#34; // unique user identifier 5 }, 6 \u0026#34;id\u0026#34;: \u0026#34;25100647-0dcc-4571-b7b4-b03e4ce72d02\u0026#34;, // unique message identifier, the same of `data.objectId` in this case 7 \u0026#34;eventType\u0026#34;: \u0026#34;Microsoft.ActiveDirectory\u0026#34;, 8 \u0026#34;subject\u0026#34;: \u0026#34;contosob2cqtofmpm.onmicrosoft.com\u0026#34;, 9 \u0026#34;dataVersion\u0026#34;: \u0026#34;1.0\u0026#34;, 10 \u0026#34;metadataVersion\u0026#34;: \u0026#34;1\u0026#34;, 11 \u0026#34;eventTime\u0026#34;: \u0026#34;2021-12-03T21:04:03.8504745Z\u0026#34;, 12 \u0026#34;topic\u0026#34;: \u0026#34;/subscriptions/{your-subscription-id}/resourceGroups/{your-resource-group}/providers/Microsoft.EventGrid/topics/{your-event-grid-topic}\u0026#34; 13 } 14] Issuing the registration event. Sending events to the topic is done using a RESTful technical profile.\n1\u0026lt;TechnicalProfile Id=\u0026#34;AAD-UserEmitRegistrationEvent\u0026#34;\u0026gt; 2 \u0026lt;DisplayName\u0026gt;Emit user registration event to Event Grid.\u0026lt;/DisplayName\u0026gt; 3 \u0026lt;Protocol Name=\u0026#34;Proprietary\u0026#34; Handler=\u0026#34;Web.TPEngine.Providers.RestfulProvider, Web.TPEngine, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null\u0026#34; /\u0026gt; 4 \u0026lt;Metadata\u0026gt; 5 \u0026lt;Item Key=\u0026#34;ServiceUrl\u0026#34;\u0026gt;{Settings:CustomerRegisteredTopicUrl}\u0026lt;/Item\u0026gt; 6 \u0026lt;Item Key=\u0026#34;AuthenticationType\u0026#34;\u0026gt;ApiKeyHeader\u0026lt;/Item\u0026gt; 7 \u0026lt;Item Key=\u0026#34;SendClaimsIn\u0026#34;\u0026gt;Body\u0026lt;/Item\u0026gt; 8 \u0026lt;Item Key=\u0026#34;ClaimUsedForRequestPayload\u0026#34;\u0026gt;userRegisterEvent\u0026lt;/Item\u0026gt; 9 \u0026lt;Item Key=\u0026#34;DefaultUserMessageIfRequestFailed\u0026#34;\u0026gt;Cannot process your request right now, please try again later.\u0026lt;/Item\u0026gt; 10 \u0026lt;/Metadata\u0026gt; 11 \u0026lt;CryptographicKeys\u0026gt; 12 \u0026lt;Key Id=\u0026#34;aeg-sas-key\u0026#34; StorageReferenceId=\u0026#34;B2C_1A_CustomerRegisteredTopicSas\u0026#34; /\u0026gt; 13 \u0026lt;/CryptographicKeys\u0026gt; 14 \u0026lt;InputClaimsTransformations\u0026gt; 15 \u0026lt;InputClaimsTransformation ReferenceId=\u0026#34;GetSystemDateTime\u0026#34; /\u0026gt; 16 \u0026lt;InputClaimsTransformation ReferenceId=\u0026#34;GenerateRegistrationEventRequest\u0026#34; /\u0026gt; 17 \u0026lt;/InputClaimsTransformations\u0026gt; 18 \u0026lt;InputClaims\u0026gt; 19 \u0026lt;InputClaim ClaimTypeReferenceId=\u0026#34;userRegisterEvent\u0026#34; /\u0026gt; 20 \u0026lt;/InputClaims\u0026gt; 21 \u0026lt;PersistedClaims\u0026gt; 22 \u0026lt;PersistedClaim ClaimTypeReferenceId=\u0026#34;systemDateTime\u0026#34; /\u0026gt; 23 \u0026lt;/PersistedClaims\u0026gt; 24 \u0026lt;UseTechnicalProfileForSessionManagement ReferenceId=\u0026#34;SM-AAD\u0026#34; /\u0026gt; 25\u0026lt;/TechnicalProfile\u0026gt; This fragment of markup translated into curl command, for more explicability, would look like this:\n1curl -X POST -H \u0026#34;aeg-sas-key: $key\u0026#34; -d \u0026#34;$event\u0026#34; $endpoint where the authentication requirements are met by the AuthenticationType metadata to which is associated the cryptographic key aeg-sas-key whose value is retrieved from the key B2C_1A_CustomerRegisteredTopicSas present in the collection of policy keys.\nTL;DR  The choice of the topic template in this example was guided by the limitations currently imposed by the RESTful technical profile regarding the possibilities of building the HTTP request, in fact for a combination of criteria it is not possible to pass information in the headers and the body of the request at the same time.\nThis makes it impossible to send towards a topic schemes of type Cloud Event since the protocol, in version 1.0 requires the presence of a mandatory header.\n \nMuch more complex is the creation of the body of the request for which it is necessary:\n use the InputClaimsTransformation add two statements inside the baggage userRegisterEvent and systemDateTime both of type string.  Finally, the technical profile has been added among the technical validation profiles of LocalAccountSignUpWithLogonEmail so that the event is issued only when a user is registered.\nUsing claim transformations. During the creation of custom criteria we could have the necessity to execute calculations, as the number of attempts of authentication, that even if very simple would result impossible without the execution of functions.\nThis requirement finds expressivity through the ClaimsTransformation whose reference of the transformations of the claims contains the complete list of the transformations usable.\nIn the example the methods GetCurrentDateTime and GenerateJson were used\n`` xml   \n The purpose of `GetSystemDateTime` is to enhance the `systemDateTime` claim. `` xml \u0026lt;ClaimsTransformation Id=\u0026quot;GenerateRegistrationEventRequest\u0026quot; TransformationMethod=\u0026quot;GenerateJson\u0026quot;\u0026gt; \u0026lt;InputClaims\u0026gt; \u0026lt;InputClaim ClaimTypeReferenceId=\u0026quot;objectId\u0026quot; TransformationClaimType=\u0026quot;0.data.objectId\u0026quot; /\u0026gt; \u0026lt;InputClaim ClaimTypeReferenceId=\u0026quot;objectId\u0026quot; TransformationClaimType=\u0026quot;0.id\u0026quot; /\u0026gt; \u0026lt;InputClaim ClaimTypeReferenceId=\u0026quot;systemDateTime\u0026quot; TransformationClaimType=\u0026quot;0.eventTime\u0026quot; /\u0026gt; \u0026lt;/InputClaims\u0026gt; \u0026lt;InputParameters\u0026gt; \u0026lt;InputParameter Id=\u0026quot;0.dataVersion\u0026quot; DataType=\u0026quot;string\u0026quot; Value=\u0026quot;1.0\u0026quot; /\u0026gt; \u0026lt;InputParameter Id=\u0026quot;0.eventType\u0026quot; DataType=\u0026quot;string\u0026quot; Value=\u0026quot;Microsoft.ActiveDirectory\u0026quot; /\u0026gt; \u0026lt;InputParameter Id=\u0026quot;0.subject\u0026quot; DataType=\u0026quot;string\u0026quot; Value=\u0026quot;{Settings:Tenant}\u0026quot; /\u0026gt; \u0026lt;/InputParameters\u0026gt; \u0026lt;OutputClaims\u0026gt; \u0026lt;OutputClaim ClaimTypeReferenceId=\u0026quot;userRegisterEvent\u0026quot; TransformationClaimType=\u0026quot;outputClaim\u0026quot; /\u0026gt; \u0026lt;/OutputClaims\u0026gt; \u0026lt;/ClaimsTransformation\u0026gt; GenerateRegistrationEventRequest has instead the burden of constructing the JSON and enhancing the userRegisterEvent claim.\nConclusions. In this article, we have seen how through Identity Experience Framework it is possible to integrate a B2C tenant with our infrastructure and open possible interesting development scenarios.\nTo do this we touched on Azure Event Grid and how to create an Event Grid Topic.\nFinally how you can manipulate attestations and use them within technical profiles.\nIf you are interested in the complete example you can find it at https://github.com/binick/samples/tree/master/src/enrich-a-jwt-token-with-ief.\n","permalink":"https://binick.github.io/2022/01/08/aadb2c-subscribe-to-user-registration-event/","summary":"In the previous post, we have seen how to enrich a JWT issued by Active Directory B2C with custom policies.\nIn that post, we talked about how it\u0026rsquo;s possible to add information external to Microsoft Graph to a JWT through the use of a Logic App and a Blob Storage.\nInstead, in this one, we will see how it is possible to create a solution that integrates Azure Active Directory B2C.","title":"Develop integrated solutions with Active Directory B2C and Azure Event Grid."},{"content":"By code coverage, we mean the action of trying to measure how much of our code has been executed by our tests. This sound like\n TL;DR\nUntested code is a broken code. Definitely a strong statement but true in a way, we don\u0026rsquo;t always manage to get enough coverage. Often this happens because we don\u0026rsquo;t have time, other times because despite having written tests we are not able to read the metrics.\n So, how we can \u0026ldquo;humanize\u0026rdquo; code coverage metrics? And how we can generate its?\nTo answer at these questions I usually use two libraries.\n coverlet-coverage / coverlet   to gather metrics, and\n danielpalme / ReportGenerator   for generate human-readable reports.\nHow can set-up coverlet? I usually include coverlet.msbuild by MSBuild .targets Files - Visual Studio | Microsoft Docs.\n For alternative ways to include coverlet into yout test project see also coverlet-coverage/coverlet: Cross platform code coverage for .NET (github.com).\nHow can set-up ReportGenerator? In keeping with above to include ReportGenerator by MSBuild .targets Files - Visual Studio | Microsoft Docs.\n Also this tool offer a various way to use it, you can find all ways onto official documentation ReportGenerator - converts coverage reports generated by coverlet.\nHow to wire-up all that? To make everything work we need to add another MSBuild file.\n And include this into your test project, something like this\n1\u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; 2 3 \u0026lt;PropertyGroup\u0026gt; 4 \u0026lt;TargetFramework\u0026gt;net5.0\u0026lt;/TargetFramework\u0026gt; 5 \u0026lt;/PropertyGroup\u0026gt; 6 7 \u0026lt;Import Project=\u0026#34;Tests.targets\u0026#34; /\u0026gt; 8 9\u0026lt;/Project\u0026gt; Now everything you are able to run dotnet test you will able to inspect and analyze something like this\nI think that is an amazing tool to understand at a glance which codes are covered and which not.\nAnd now, how I can put it into Azure DevOps pipeline? It would be nice if this report came was published into the Build pipeline report, don\u0026rsquo;t you think? Maybe even include branch policies for it.\nWell that\u0026rsquo;s possible by use Publish Code Coverage Results task, something like this:\n1- task:PublishCodeCoverageResults@12displayName:Publish Code Coverage Results3inputs:4codeCoverageTool:\u0026#39;cobertura\u0026#39;5summaryFileLocation:\u0026#39;$(Build.SourcesDirectory)/artifacts/TestResults/$(_BuildConfig)/Reports/Summary/Cobertura.xml\u0026#39;6continueOnError:true7condition:always()We notice the summaryFileLocation argument, this means that we will push only one file to Azure DevOps why?\nOne unwrite note of Publish Code Coverage Results task or limitation, I don\u0026rsquo;t know, is that the sum of covered lines, when we publish more reports, is take from the first file This results in an unreliable result.\nTo fix that problem we can marge multiple reports into a summary reports so that can be publish it only one. One way to make it is the follow\n and run MSBuild project into the pipeline with\n1- script:dotnet msbuild SummaryReportGenerator.proj /p:Configuration=$(Configuration)2name:GenerateCodeCoverageSummary3displayName:Generate code coverage summaryOnce you\u0026rsquo;ve done this the sum of covered lines on Build pipeline will true.\n","permalink":"https://binick.github.io/2021/01/02/azure-devops-code-coverage/","summary":"By code coverage, we mean the action of trying to measure how much of our code has been executed by our tests. This sound like\n TL;DR\nUntested code is a broken code. Definitely a strong statement but true in a way, we don\u0026rsquo;t always manage to get enough coverage. Often this happens because we don\u0026rsquo;t have time, other times because despite having written tests we are not able to read the metrics.","title":"How to include code coverage in Azure DevOps pipeline?"},{"content":"Sometimes we have been forced to work with JSON stored on table columns, it will have happened to you too!\nIn this post, I want to show you how to work with that using EntityFramework Core\n dotnet / efcore   Clearly this is one of many possible ways.\nWe could talk for a long time about the choice to store JSON into RDBMS is a good or bad choice, but the intent of this post isn\u0026rsquo;t making a rant.\nOk, well. First of all, take a look to JSON that we want to persist into a table column\n Our application has a requirement that makes it necessary to query the database with the name of the retailer that has stored in the JSON.\nThe retailer is the one who has the car we want to rent. The car is represented by the class\n The Car entity has a property public string NameOfRetailer { get; } that is computed by the Computed columns functionality.\n With this instruction efcore will inflate property with value returned by JSON_VALUE(Metadata, '$.Retailer.Name') expression, for more information about JSON_VALUE see at JSON_VALUE\nTo make it work, we need to persist the JSON into table column Metadata.\nWe can use the other useful Value conversions functionality of efcore.\n  Now, after that model configurations we are able to resolve this simple query var car = await context.Cars.MaterializeAsync(car =\u0026gt; car.NameOfRetailer == \u0026quot;Car Auto Orvieto\u0026quot;).ConfigureAwait(false); without materialize the entire dataset on the client. 🚀\nIf you want to learn more you can find the sample on my github repo ef-core-json\nHappy coding! 🐱‍👤\n","permalink":"https://binick.github.io/2020/10/22/sqlserver-efcore-json/","summary":"Sometimes we have been forced to work with JSON stored on table columns, it will have happened to you too!\nIn this post, I want to show you how to work with that using EntityFramework Core\n dotnet / efcore   Clearly this is one of many possible ways.\nWe could talk for a long time about the choice to store JSON into RDBMS is a good or bad choice, but the intent of this post isn\u0026rsquo;t making a rant.","title":"SQL Server, EF Core, JSON 👀"},{"content":"","permalink":"https://binick.github.io/case-study/","summary":"","title":""},{"content":"From the first basics in Pascal through Qt and QML, C# and SQL. Nicola is a full-stack code cleanliness maniac driven by the five SOLID principles.\nHis workbench is managed by chocolatey.org, wsl, and pwsh.\nPassionate about containerization, cloud, and CI/CD processes he is also a strong supporter of TDD, throughout his career he has worked in both web-based and desktop/mobile environments.\nHe strongly believes in OSS to which he tries to contribute regularly.\n","permalink":"https://binick.github.io/about-me/","summary":"About me","title":"About me"},{"content":"Postal Bird is a company that offers sheetfed and web offset printing services as well as services to support its customers for integrated communication.\nThe company\u0026rsquo;s decades of experience have been engineered into a software platform by its IT department and other third parties who have implemented satellite parts. All to offer its customers the best possible user experience.\nOne of the tools offered to customers for the management of integrated communication is represented by a web portal that has been very successful but has begun to show some slowdowns due to the increasing volumes of data managed.\n","permalink":"https://binick.github.io/case-studies/postalbird/","summary":"Postal Bird is a company that offers sheetfed and web offset printing services as well as services to support its customers for integrated communication.\nThe company\u0026rsquo;s decades of experience have been engineered into a software platform by its IT department and other third parties who have implemented satellite parts. All to offer its customers the best possible user experience.\nOne of the tools offered to customers for the management of integrated communication is represented by a web portal that has been very successful but has begun to show some slowdowns due to the increasing volumes of data managed.","title":"Overview of Postal Bird"}]