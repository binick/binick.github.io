[{"content":"Some time ago I was faced to need to enrich the JSON Web Token issued by Azure Active Directory B2C with information present on an external system.\nTo create this scenario, I leveraged the customization options offered by custom criteria.\nIntroduction to Identity Experience Framework. Identity Experience Framework represents the orchestration context of an interaction flow with Azure AD B2C. Custom criteria represent its expressiveness and allow us, developers, to define the internal flow through a series of one or more·Ω∫ markup files.\nHere you can find the About the basics.\nRetrieve Information. In that case, the information was being exposed from a web server exposed behind API Management, for simplicity, I replicated the endpoint with a Logic App. This is its definition:\nAs you can imagine its task is to return the information contained within a blob. In this case, the blob contains the information that will be exposed as JWT claims.\nIn particular, given the objectId which in this case represents the user, a json with this format will be returned:\n1{ 2 \u0026#34;location\u0026#34;: \u0026#34;IT\u0026#34;, 3 \u0026#34;work-teams\u0026#34;: [ 4 \u0026#34;Star\u0026#34;, 5 \u0026#34;Eco\u0026#34; 6 ] 7} Create our custom policy. The objective of the example is simple and consists in adding to the flow of registration and authentication the two attestations respectively with the names loc for the location and wt for the work teams.\n NOTE\nEvery time that we have the necessity to use IEF, it is advisable to start from one of the templates that Microsoft puts to disposition in the starter pack, in this example LocalAccounts has been used.\n The first thing to do then is to edit the ClaimsSchema by adding the ClaimType\n1\u0026lt;ClaimType Id=\u0026#34;location\u0026#34;\u0026gt; 2 \u0026lt;DisplayName\u0026gt;Location\u0026lt;/DisplayName\u0026gt; 3 \u0026lt;DataType\u0026gt;string\u0026lt;/DataType\u0026gt; 4 \u0026lt;DefaultPartnerClaimTypes\u0026gt; 5 \u0026lt;Protocol Name=\u0026#34;OAuth2\u0026#34; PartnerClaimType=\u0026#34;loc\u0026#34; /\u0026gt; 6 \u0026lt;Protocol Name=\u0026#34;OpenIdConnect\u0026#34; PartnerClaimType=\u0026#34;loc\u0026#34; /\u0026gt; 7 \u0026lt;Protocol Name=\u0026#34;SAML2\u0026#34; PartnerClaimType=\u0026#34;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/location\u0026#34; /\u0026gt; 8 \u0026lt;/DefaultPartnerClaimTypes\u0026gt; 9 \u0026lt;UserHelpText\u0026gt;Your country (express with ISO 3166-1 alpha-2 format).\u0026lt;/UserHelpText\u0026gt; 10\u0026lt;/ClaimType\u0026gt; 1\u0026lt;ClaimType Id=\u0026#34;workTeams\u0026#34;\u0026gt; 2 \u0026lt;DisplayName\u0026gt;Groups\u0026lt;/DisplayName\u0026gt; 3 \u0026lt;DataType\u0026gt;stringCollection\u0026lt;/DataType\u0026gt; 4 \u0026lt;DefaultPartnerClaimTypes\u0026gt; 5 \u0026lt;Protocol Name=\u0026#34;OAuth2\u0026#34; PartnerClaimType=\u0026#34;wt\u0026#34; /\u0026gt; 6 \u0026lt;Protocol Name=\u0026#34;OpenIdConnect\u0026#34; PartnerClaimType=\u0026#34;wt\u0026#34; /\u0026gt; 7 \u0026lt;Protocol Name=\u0026#34;SAML2\u0026#34; PartnerClaimType=\u0026#34;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/workteams\u0026#34; /\u0026gt; 8 \u0026lt;/DefaultPartnerClaimTypes\u0026gt; 9 \u0026lt;UserHelpText\u0026gt;Work teams you belong to.\u0026lt;/UserHelpText\u0026gt; 10\u0026lt;/ClaimType\u0026gt; The PartnerClaimType attribute of the Protocol element represents the name that will be used to identify the claim based on the supported protocol.\nAt this point we could value them through an HTTP call to the Logic App. To do this we would need to create a TechnicalProfile, which in this case will be defined as follows:\n1\u0026lt;TechnicalProfile Id=\u0026#34;AAD-UserDetailsReadUsingObjectId\u0026#34;\u0026gt; 2 \u0026lt;DisplayName\u0026gt;Provide identity customer details\u0026lt;/DisplayName\u0026gt; 3 \u0026lt;Protocol Name=\u0026#34;Proprietary\u0026#34; Handler=\u0026#34;Web.TPEngine.Providers.RestfulProvider, Web.TPEngine, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null\u0026#34; /\u0026gt; 4 \u0026lt;Metadata\u0026gt; 5 \u0026lt;Item Key=\u0026#34;ServiceUrl\u0026#34;\u0026gt;https://localhost/customers/{objectId}\u0026lt;/Item\u0026gt; 6 \u0026lt;Item Key=\u0026#34;AuthenticationType\u0026#34;\u0026gt;None\u0026lt;/Item\u0026gt; 7 \u0026lt;Item Key=\u0026#34;SendClaimsIn\u0026#34;\u0026gt;Url\u0026lt;/Item\u0026gt; 8 \u0026lt;/Metadata\u0026gt; 9 \u0026lt;InputClaims\u0026gt; 10 \u0026lt;InputClaim ClaimTypeReferenceId=\u0026#34;objectId\u0026#34; Required=\u0026#34;true\u0026#34; /\u0026gt; 11 \u0026lt;/InputClaims\u0026gt; 12 \u0026lt;OutputClaims\u0026gt; 13 \u0026lt;OutputClaim ClaimTypeReferenceId=\u0026#34;workTeams\u0026#34; PartnerClaimType=\u0026#34;work-teams\u0026#34; /\u0026gt; 14 \u0026lt;OutputClaim ClaimTypeReferenceId=\u0026#34;location\u0026#34; PartnerClaimType=\u0026#34;location\u0026#34; /\u0026gt; 15 \u0026lt;/OutputClaims\u0026gt; 16 \u0026lt;IncludeTechnicalProfile ReferenceId=\u0026#34;AAD-UserReadUsingObjectId\u0026#34; /\u0026gt; 17\u0026lt;/TechnicalProfile\u0026gt; Where the Protocol element is used to identify the type of TechnicalProfile whose behavior is defined through the Metadata element.\nAs we saw earlier in the Retrieve Information section, it needs objectId so:\n ServiceUrl identifies the HTTP endpoint where inputs can be passed via the placeholder {claim} SendClaimsIn instructs the Web.TPEngine.Providers.RestfulProvider assembly on the place in which the claims must be passed  OutputClaims instead has the task to execute the mapping between the properties contained in the answer and the ClaimType defined previously.\nThe last thing to make is to modify the RelyingParty adding to the property OutputClaims the references to the objects\n1\u0026lt;OutputClaim ClaimTypeReferenceId=\u0026#34;location\u0026#34; /\u0026gt; and\n1\u0026lt;OutputClaim ClaimTypeReferenceId=\u0026#34;workTeams\u0026#34; /\u0026gt; to finally achieve\nTroubleshooting and development help. Unavoidably during development, there might be errors, errors that Azure AD B2C reports as within the redirect URL and that https://jwt.ms transforms like this\nFortunately, troubleshooting operations can come to our aid Application Insights, for configuration operations, you can refer to this guide Collect Azure Active Directory log B2C with Application Insights.\nAnother difficulty lies in the fact that the definition of custom criteria uses a markup language. Who can develop without autocompletion anymore?\nThe Azure AD B2C community has thought about this problem by making an extension for VS code.\nIt has also created several example scenarios that may come in handy Azure Active Directory B2C: Custom CIAM User Journeys.\nFinally, if you are interested in the complete example, you can find it at the following address https://github.com/binick/samples/tree/master/src/enrich-a-jwt-token-with-ief.\n","permalink":"https://binick.github.io/posts/2021-12-25_enrich-a-jwt-token-with-ief/","summary":"Some time ago I was faced to need to enrich the JSON Web Token issued by Azure Active Directory B2C with information present on an external system.\nTo create this scenario, I leveraged the customization options offered by custom criteria.\nIntroduction to Identity Experience Framework. Identity Experience Framework represents the orchestration context of an interaction flow with Azure AD B2C. Custom criteria represent its expressiveness and allow us, developers, to define the internal flow through a series of one or more·Ω∫ markup files.","title":"How to enrich a JWT issued by Active Directory B2C with custom policies."},{"content":"By code coverage, we mean the action of trying to measure how much of our code has been executed by our tests. This sound like\n TL;DR\nUntested code is a broken code. Definitely a strong statement but true in a way, we don\u0026rsquo;t always manage to get enough coverage. Often this happens because we don\u0026rsquo;t have time, other times because despite having written tests we are not able to read the metrics.\n So, how we can \u0026ldquo;humanize\u0026rdquo; code coverage metrics? And how we can generate its?\nTo answer at these questions I usually use two libraries.\n coverlet-coverage / coverlet   to gather metrics, and\n danielpalme / ReportGenerator   for generate human-readable reports.\nHow can set-up coverlet? I usually include coverlet.msbuild by MSBuild .targets Files - Visual Studio | Microsoft Docs.\n For alternative ways to include coverlet into yout test project see also coverlet-coverage/coverlet: Cross platform code coverage for .NET (github.com).\nHow can set-up ReportGenerator? In keeping with above to include ReportGenerator by MSBuild .targets Files - Visual Studio | Microsoft Docs.\n Also this tool offer a various way to use it, you can find all ways onto official documentation ReportGenerator - converts coverage reports generated by coverlet.\nHow to wire-up all that? To make everything work we need to add another MSBuild file.\n And include this into your test project, something like this\n1\u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; 2 3 \u0026lt;PropertyGroup\u0026gt; 4 \u0026lt;TargetFramework\u0026gt;net5.0\u0026lt;/TargetFramework\u0026gt; 5 \u0026lt;/PropertyGroup\u0026gt; 6 7 \u0026lt;Import Project=\u0026#34;Tests.targets\u0026#34; /\u0026gt; 8 9\u0026lt;/Project\u0026gt; Now everything you are able to run dotnet test you will able to inspect and analyze something like this\nI think that is an amazing tool to understand at a glance which codes are covered and which not.\nAnd now, how I can put it into Azure DevOps pipeline? It would be nice if this report came was published into the Build pipeline report, don\u0026rsquo;t you think? Maybe even include branch policies for it.\nWell that\u0026rsquo;s possible by use Publish Code Coverage Results task, something like this:\n1- task:PublishCodeCoverageResults@12displayName:Publish Code Coverage Results3inputs:4codeCoverageTool:\u0026#39;cobertura\u0026#39;5summaryFileLocation:\u0026#39;$(Build.SourcesDirectory)/artifacts/TestResults/$(_BuildConfig)/Reports/Summary/Cobertura.xml\u0026#39;6continueOnError:true7condition:always()We notice the summaryFileLocation argument, this means that we will push only one file to Azure DevOps why?\nOne unwrite note of Publish Code Coverage Results task or limitation, I don\u0026rsquo;t know, is that the sum of covered lines, when we publish more reports, is take from the first file This results in an unreliable result.\nTo fix that problem we can marge multiple reports into a summary reports so that can be publish it only one. One way to make it is the follow\n and run MSBuild project into the pipeline with\n1- script:dotnet msbuild SummaryReportGenerator.proj /p:Configuration=$(Configuration)2name:GenerateCodeCoverageSummary3displayName:Generate code coverage summaryOnce you\u0026rsquo;ve done this the sum of covered lines on Build pipeline will true.\n","permalink":"https://binick.github.io/posts/2021-01-02_azure-devops-code-coverage/","summary":"By code coverage, we mean the action of trying to measure how much of our code has been executed by our tests. This sound like\n TL;DR\nUntested code is a broken code. Definitely a strong statement but true in a way, we don\u0026rsquo;t always manage to get enough coverage. Often this happens because we don\u0026rsquo;t have time, other times because despite having written tests we are not able to read the metrics.","title":"How to include code coverage in Azure DevOps pipeline?"},{"content":"Sometimes we have been forced to work with JSON stored on table columns, it will have happened to you too!.\nIn this post I want to show you how work with that using EntityFramework Core\n dotnet / efcore   Clearly this is one of many possible ways.\nWe could talk for a long time about the choice to store JSON into RDBMS is a good or bad choice, but the intent of this post isn\u0026rsquo;t make a rant.\nOk, well. First of all take a look to JSON that we want to persist into a table column\n Our application has a requirement that makes it necessary to query the database with the name of the retailer that has stored in the JSON.\nThe retailer is the one who has the car we want to rent. The car is represented by the class\n The Car entity has a property public string NameOfRetailer { get; } that is computed by the Computed columns functionality.\n With this instruction efcore will inflate property with value returned by JSON_VALUE(Metadata, '$.Retailer.Name') expression, for more information about JSON_VALUE see at JSON_VALUE\nTo make it work, we need to persist the JSON into table column Metadata.\nWe can use the other usefull Value conversions functionality of efcore.\n  Now, after that model configurations we are able to resolve this simple query var car = await context.Cars.MaterializeAsync(car =\u0026gt; car.NameOfRetailer == \u0026quot;Car Auto Orvieto\u0026quot;).ConfigureAwait(false); without materialize the entire dataset on the client. üöÄ\nIf you want to learn more you can find the sample on my github repo ef-core-json\nHappy coding!üê±‚Äçüë§\n","permalink":"https://binick.github.io/posts/2020-10-22_sqlserver-efcore-json/","summary":"Sometimes we have been forced to work with JSON stored on table columns, it will have happened to you too!.\nIn this post I want to show you how work with that using EntityFramework Core\n dotnet / efcore   Clearly this is one of many possible ways.\nWe could talk for a long time about the choice to store JSON into RDBMS is a good or bad choice, but the intent of this post isn\u0026rsquo;t make a rant.","title":"SqlServer, EFCore, JSON üëÄ"},{"content":"About me From the first basics in Pascal through Qt and QML, C# and SQL. Nicola is a full-stack code cleanliness maniac driven by the five SOLID principles.\nHis workbench is managed by chocolatey.org, wsl, and pwsh.\nPassionate about containerization, cloud, and CI/CD processes he is also a strong supporter of TDD, throughout his career he has worked in both web-based and desktop/mobile environments.\nHe strongly believes in OSS to which he tries to contribute regularly.\n","permalink":"https://binick.github.io/about-me/","summary":"About me From the first basics in Pascal through Qt and QML, C# and SQL. Nicola is a full-stack code cleanliness maniac driven by the five SOLID principles.\nHis workbench is managed by chocolatey.org, wsl, and pwsh.\nPassionate about containerization, cloud, and CI/CD processes he is also a strong supporter of TDD, throughout his career he has worked in both web-based and desktop/mobile environments.\nHe strongly believes in OSS to which he tries to contribute regularly.","title":""}]